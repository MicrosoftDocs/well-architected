---
title: Data design for AI workloads on Azure
description: Data layout considerations for running AI workloads.
author: PageWriter-MSFT
ms.author: prwilk
ms.date: 04/15/2024
ms.topic: conceptual
ms.service: waf
ms.subservice: waf-workload-ai
---

# Data design for AI workloads on Azure

- Structuring data in order to work with large language models
- Types of search
  - Vector
  - Full Text
  - Keywords
- Chunking and Ingestion of data
- Optimizing data stores for how AI models search them
- Protecting user data
- Monitoring (required)

## Optimize indexing techniques

 There might be slow response times when querying a vector database. The root cause is often inadequate indexing.

## Searching the database

## Types of data

There could be training data is used to build and train machine learning models. There's also  user data that includes information generated by users during interactions with the application. Each type has it's own requirements (prpocessing, storage, and retrieval).

## Elisabeth's (SME & area lead) seed material

TODO

## Jose's (Azure Patterns & Practices engineering) seed material

Data design

RAG Pattern - Data stores for Augmented phase
  - What are best practices and recommendations for designing the Data store and the indexes for maximum accuracy
  - Data preparation: OCR, lower-casing, synonyms, stemming, entity recognition, enrichments, chunking, embeddings, vectors
    ○ Data enrichments example: adding entity attributes after recognizing them. Addresses, classifications, important attributes
    ○ Q: vector index vs keyword index, and the data preparation techniques. Example: synonyms and stemming may not(?) be as important b/c of the semantic proximity
  - Types of index/queries and recommendations (vector, keyword, combined, etc.). Tradeoffs between accuracy and performance
  - Index design and Searching
  - Data store options and how it drives design. How to decide which data store to use
  - One [augmentation] data store vs multiple. 
  - Integrating external systems and data stores, real time lookups and batch integration.
  - Tradeoffs between cost, performance, and functionality (e.g. accuracy)
  - Security and privacy considerations. IP implications
  - Describe iterative process to optimize data design. Basic example: load data with initial schema, execute common queries, tweak enrichments/schema/queries, and try again. Touch on testing(?)
  
Training data

  - Data lakes: storage, security and cost considerations. Move/aggregate data vs remote access. 
  - Versioning and schemas
  - Data pipelines: data cleansing, privacy and security, correlating and deduping data across enterprise systems 
  - Re-training. Feeding data from prod systems
  
Other

  - Q: how do we address relationship between data platform and design? At some point, maybe after some initial data design, the platform is selected and it becomes a key factor in the design. In other cases the platform may be already selected


## Chad's (Azure Patterns & Practices engineering) seed material

For training, inference, and grounding data. Raw data ultimately needs to get converted into vectors. Turning raw data into numeric feature vectors is commonly known as "feature engineering." In order to convert data into features, that raw data needs to be collected, cleaned, organized, and potentially enriched (labeled, etc).

### Dynamic/online training (or updating grounding data)

This is where the model stays up to date with new data as it comes in. You do this to avoid staleness issues in the model. This helps smooth out seasonal changes in data (for example). You do need to monitor input, because a bad run (missing data, corrupt data) can impact the model right away. Automated monitoring and testing the training jobs are critical since the window is expected to be short between model releases -- automation will be key. A bug in the online training can cause production issues.

This model is shipping to production frequently.

Great for personalization experiences (using daily user trends)

### Static/offline training (or generating grounding data or fine-tuning)

Best for data that doesn't need more realtime inclusion. The model (or grounding data) will be stale until refreshed. While you do need to monitor training, it happens less frequently so doesn't need to be fully automated. You still need to scrub data and monitor input (distributions, quality, etc).  For example, you wouldn't want to train on winter activity but try to predict summer behavior

This model is shipping to production infrequently.

### Offline inferencing

We pre-calculate predictions and store them in look-up tables. Look up has minimal latency. This requires predictions for all possible inputs. Allows us to validate (or even tweak) predictions before being used in production.

TODO: What's our data modeling guidance here?

### Online inferencing

We don't pre-calculate predictions, and call into the model to answer questions right away (data "is" the model).

TODO: What's our data design guidance here, if any?

### Collecting datasets

- TODO: What are the recommendations for gathering training or grounding data?

### Cleaning and organizing data

- TODO: What are the recommendations for cleaning and organizing data for training or grounding consumption?

### Building training sets

- TODO: What are the recommendations for building training datasets?

- Break data into training, validation, and test (cross-validation) sets.

- When do you use cross-validation vs when do you split data between train/validation?

### Vectorization

Any topics to talk about here?

- sparse representation for large data sets (vector storage efficiency)
- Ensure numbers that should be treated as strings are vectorized accordingly (e.g. a zipcode)
- one-hot encoding: <https://en.wikipedia.org/wiki/One-hot>
  - binary vector
  - small datasets with limited number of classes
  - high memory usage

### Handling imbalanced data

Not all data sets are balanced, in that sometimes its hard to get samples of representative data -- this is especially important for classification usage. For example, in a fraud detection solution, you might not have access to sufficient training data. In that case, you might need to oversample the minority data or under sample the majority data. Synthetic Minority Oversampling Technique (SMOTE) - <https://learn.microsoft.com/azure/machine-learning/component-reference/smote>
